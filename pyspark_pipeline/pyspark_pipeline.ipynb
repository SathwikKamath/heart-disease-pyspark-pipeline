{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "724c3257-c4e8-4228-9aae-507d8cc9454e",
   "metadata": {},
   "source": [
    "# Sathwik Kamath \n",
    "### E-mail:kamathsathwik18@gmail.com\n",
    "### LinkedIn: www.linkedin.com/in/sathwik-kamath\n",
    "### Github: https://github.com/SathwikKamath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e8337de3-7b18-4eb2-a4aa-aca0cee153ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Set Java and Hadoop paths (needed for Spark to run locally on Windows)\n",
    "os.environ[\"JAVA_HOME\"] = \"D:/openjdk-11.0.0.2_windows-x64/jdk-11.0.0.2\"\n",
    "os.environ[\"HADOOP_HOME\"] = \"D:/hadoop\"\n",
    "\n",
    "# Create Spark Session with MySQL JDBC Connector\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"HeartDiseasePipeline\")\n",
    "    .master(\"local[*]\")  # Use all cores of your CPU\n",
    "    .config(\"spark.jars\", \"file:///D:/mysql-connector-j-9.3.0/mysql-connector-j-9.3.0.jar\")  # MySQL driver\n",
    "    .config(\"spark.driver.extraClassPath\", \"D:/mysql-connector-j-9.3.0/mysql-connector-j-9.3.0.jar\")\n",
    "    .getOrCreate()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "727f4658-c917-45cd-92c4-32439b528b6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Schema of the data:\n",
      "root\n",
      " |-- age: double (nullable = true)\n",
      " |-- sex: double (nullable = true)\n",
      " |-- cp: double (nullable = true)\n",
      " |-- trestbps: double (nullable = true)\n",
      " |-- chol: double (nullable = true)\n",
      " |-- fbs: double (nullable = true)\n",
      " |-- restecg: double (nullable = true)\n",
      " |-- thalach: double (nullable = true)\n",
      " |-- exang: double (nullable = true)\n",
      " |-- oldpeak: double (nullable = true)\n",
      " |-- slope: double (nullable = true)\n",
      " |-- ca: double (nullable = true)\n",
      " |-- thal: double (nullable = true)\n",
      " |-- target: long (nullable = true)\n",
      "\n",
      "\n",
      "First 5 rows:\n",
      "+----+---+---+--------+-----+---+-------+-------+-----+-------+-----+---+----+------+\n",
      "| age|sex| cp|trestbps| chol|fbs|restecg|thalach|exang|oldpeak|slope| ca|thal|target|\n",
      "+----+---+---+--------+-----+---+-------+-------+-----+-------+-----+---+----+------+\n",
      "|63.0|1.0|1.0|   145.0|233.0|1.0|    2.0|  150.0|  0.0|    2.3|  3.0|0.0| 6.0|     0|\n",
      "|67.0|1.0|4.0|   160.0|286.0|0.0|    2.0|  108.0|  1.0|    1.5|  2.0|3.0| 3.0|     2|\n",
      "|67.0|1.0|4.0|   120.0|229.0|0.0|    2.0|  129.0|  1.0|    2.6|  2.0|2.0| 7.0|     1|\n",
      "|37.0|1.0|3.0|   130.0|250.0|0.0|    0.0|  187.0|  0.0|    3.5|  3.0|0.0| 3.0|     0|\n",
      "|41.0|0.0|2.0|   130.0|204.0|0.0|    2.0|  172.0|  0.0|    1.4|  1.0|0.0| 3.0|     0|\n",
      "+----+---+---+--------+-----+---+-------+-------+-----+-------+-----+---+----+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# MySQL database details\n",
    "jdbc_url = \"jdbc:mysql://localhost:3306/heart_db\"\n",
    "props = {\"user\": \"root\", \"password\": \"Kamath@2001\"}\n",
    "\n",
    "# Read data from MySQL table 'heart_data'\n",
    "df = spark.read.jdbc(url=jdbc_url, table=\"cleveland_heart_disease\", properties=props)\n",
    "\n",
    "# Display schema and first few rows\n",
    "print(\"\\nSchema of the data:\")\n",
    "df.printSchema()\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "df.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e366d6cd-412a-4abb-ad6b-723d5e69e3a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+---+--------+-----+---+-------+-------+-----+-------+-----+---+----+------+\n",
      "| age|sex| cp|trestbps| chol|fbs|restecg|thalach|exang|oldpeak|slope| ca|thal|target|\n",
      "+----+---+---+--------+-----+---+-------+-------+-----+-------+-----+---+----+------+\n",
      "|63.0|1.0|1.0|   145.0|233.0|1.0|    2.0|  150.0|  0.0|    2.3|  3.0|0.0| 6.0|     0|\n",
      "|67.0|1.0|4.0|   160.0|286.0|0.0|    2.0|  108.0|  1.0|    1.5|  2.0|3.0| 3.0|     2|\n",
      "|67.0|1.0|4.0|   120.0|229.0|0.0|    2.0|  129.0|  1.0|    2.6|  2.0|2.0| 7.0|     1|\n",
      "|37.0|1.0|3.0|   130.0|250.0|0.0|    0.0|  187.0|  0.0|    3.5|  3.0|0.0| 3.0|     0|\n",
      "|41.0|0.0|2.0|   130.0|204.0|0.0|    2.0|  172.0|  0.0|    1.4|  1.0|0.0| 3.0|     0|\n",
      "|56.0|1.0|2.0|   120.0|236.0|0.0|    0.0|  178.0|  0.0|    0.8|  1.0|0.0| 3.0|     0|\n",
      "|62.0|0.0|4.0|   140.0|268.0|0.0|    2.0|  160.0|  0.0|    3.6|  3.0|2.0| 3.0|     3|\n",
      "|57.0|0.0|4.0|   120.0|354.0|0.0|    0.0|  163.0|  1.0|    0.6|  1.0|0.0| 3.0|     0|\n",
      "|63.0|1.0|4.0|   130.0|254.0|0.0|    2.0|  147.0|  0.0|    1.4|  2.0|1.0| 7.0|     2|\n",
      "|53.0|1.0|4.0|   140.0|203.0|1.0|    2.0|  155.0|  1.0|    3.1|  3.0|0.0| 7.0|     1|\n",
      "|57.0|1.0|4.0|   140.0|192.0|0.0|    0.0|  148.0|  0.0|    0.4|  2.0|0.0| 6.0|     0|\n",
      "|56.0|0.0|2.0|   140.0|294.0|0.0|    2.0|  153.0|  0.0|    1.3|  2.0|0.0| 3.0|     0|\n",
      "|56.0|1.0|3.0|   130.0|256.0|1.0|    2.0|  142.0|  1.0|    0.6|  2.0|1.0| 6.0|     2|\n",
      "|44.0|1.0|2.0|   120.0|263.0|0.0|    0.0|  173.0|  0.0|    0.0|  1.0|0.0| 7.0|     0|\n",
      "|52.0|1.0|3.0|   172.0|199.0|1.0|    0.0|  162.0|  0.0|    0.5|  1.0|0.0| 7.0|     0|\n",
      "|57.0|1.0|3.0|   150.0|168.0|0.0|    0.0|  174.0|  0.0|    1.6|  1.0|0.0| 3.0|     0|\n",
      "|48.0|1.0|2.0|   110.0|229.0|0.0|    0.0|  168.0|  0.0|    1.0|  3.0|0.0| 7.0|     1|\n",
      "|54.0|1.0|4.0|   140.0|239.0|0.0|    0.0|  160.0|  0.0|    1.2|  1.0|0.0| 3.0|     0|\n",
      "|48.0|0.0|3.0|   130.0|275.0|0.0|    0.0|  139.0|  0.0|    0.2|  1.0|0.0| 3.0|     0|\n",
      "|49.0|1.0|2.0|   130.0|266.0|0.0|    0.0|  171.0|  0.0|    0.6|  1.0|0.0| 3.0|     0|\n",
      "+----+---+---+--------+-----+---+-------+-------+-----+-------+-----+---+----+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Removing the missing value\n",
    "df_dropna= df.dropna()\n",
    "df_dropna.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52c4bc49-732f-444a-b72b-790c945dd535",
   "metadata": {},
   "source": [
    "# Apply StandardScaler to scale numeric features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "260e2b0e-f9da-4575-a2f4-c84ca74540e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler, StandardScaler\n",
    "\n",
    "# 1️⃣ Categorical columns to encode\n",
    "categorical_cols = [\"cp\", \"thal\", \"slope\"]\n",
    "\n",
    "# Index categorical columns (convert categories → numeric indices)\n",
    "indexers = [StringIndexer(inputCol=col, outputCol=col+\"_index\") for col in categorical_cols]\n",
    "\n",
    "# One-hot encode the indexed columns\n",
    "encoders = [OneHotEncoder(inputCol=col+\"_index\", outputCol=col+\"_ohe\") for col in categorical_cols]\n",
    "\n",
    "# 2️⃣ Numeric columns to scale\n",
    "numeric_cols = [\"age\", \"trestbps\", \"chol\", \"thalach\", \"oldpeak\"]\n",
    "\n",
    "# Assemble numeric features before scaling\n",
    "numeric_assembler = VectorAssembler(inputCols=numeric_cols, outputCol=\"numeric_features\")\n",
    "\n",
    "# Apply StandardScaler to scale numeric features\n",
    "scaler = StandardScaler(inputCol=\"numeric_features\", outputCol=\"scaled_numeric_features\")\n",
    "\n",
    "# 3️⃣ Combine everything into final features\n",
    "assembler_inputs = [col+\"_ohe\" for col in categorical_cols] + [\"scaled_numeric_features\"]\n",
    "\n",
    "final_assembler = VectorAssembler(inputCols=assembler_inputs, outputCol=\"features\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2baf98e-c918-4c6f-be3b-5f2592e0a6bf",
   "metadata": {},
   "source": [
    "# Feature Engineering (PySpark)\n",
    "## Sections:\n",
    "## 0) Imports & quick checks\n",
    "## 1) StringIndex categorical columns\n",
    "## 2) OneHotEncode indexed columns\n",
    "## 3) Assemble numeric columns (pre-scaling)\n",
    "## 4) Standard scale numeric features\n",
    "## 5) Final VectorAssembler -> \"features\"\n",
    "## 6) Build Pipeline, fit & transform, inspect results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "55cd4540-0ffc-4975-83d4-ec06d6968e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0) Imports & quick checks\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler, StandardScaler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql.functions import col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4d8f9c61-83e0-4377-90bf-a11b85d88cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) StringIndex categorical columns\n",
    "# - create indexer objects for each categorical column.\n",
    "# - handleInvalid=\"keep\" avoids pipeline failure for unseen categories.\n",
    "categorical_cols = [\"cp\", \"thal\", \"slope\"]\n",
    "indexers = [\n",
    "    StringIndexer(inputCol=cat, outputCol=f\"{cat}_index\", handleInvalid=\"keep\")\n",
    "    for cat in categorical_cols\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "be612f1a-34ab-46d0-ad7f-ec89c3fee204",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) OneHotEncode indexed columns\n",
    "# - convert each index column into an OHE vector column.\n",
    "# - output columns will be like \"cp_ohe\", \"thal_ohe\", \"slope_ohe\".\n",
    "encoders = [\n",
    "    OneHotEncoder(inputCol=f\"{cat}_index\", outputCol=f\"{cat}_ohe\", handleInvalid=\"keep\")\n",
    "    for cat in categorical_cols\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4b4ff73c-9af2-459b-bc02-bfa1f73d76c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) Assemble numeric columns (pre-scaling)\n",
    "# - combine raw numeric columns into single vector \"numeric_features\".\n",
    "numeric_cols = [\"age\", \"trestbps\", \"chol\", \"thalach\", \"oldpeak\"]\n",
    "numeric_assembler = VectorAssembler(inputCols=numeric_cols, outputCol=\"numeric_features\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f4856b17-7809-40d0-b6d6-653e70cc7b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4) Standard scale numeric features\n",
    "# - StandardScaler standardizes (mean=0, std=1). withMean=True centers the data.\n",
    "scaler = StandardScaler(\n",
    "    inputCol=\"numeric_features\",\n",
    "    outputCol=\"scaled_numeric_features\",\n",
    "    withMean=True,\n",
    "    withStd=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9071a17d-0727-4171-931a-c35c7d184ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5) Final VectorAssembler -> \"features\"\n",
    "# - combine categorical OHE vectors + scaled numeric vector into one \"features\" vector.\n",
    "assembler_inputs = [f\"{cat}_ohe\" for cat in categorical_cols] + [\"scaled_numeric_features\"]\n",
    "final_assembler = VectorAssembler(inputCols=assembler_inputs, outputCol=\"features\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "03234186-351b-446a-af71-f0e2b125f22b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6) Build Pipeline, fit & transform, inspect results\n",
    "stages = indexers + encoders + [numeric_assembler, scaler, final_assembler]\n",
    "pipeline = Pipeline(stages=stages)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c290cd32-e9d6-47e5-bf44-97bf856cf628",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the pipeline (learns index mappings and scaler stats) then transform the DataFrame\n",
    "pipeline_model = pipeline.fit(df_dropna)\n",
    "processed_df = pipeline_model.transform(df_dropna)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3bc3861d-c37e-4b96-bdfa-0781389af763",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+-------------+----+----------+-------------+-----+-----------+-------------+----------------------------+----------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------+------+\n",
      "|cp |cp_index|cp_ohe       |thal|thal_index|thal_ohe     |slope|slope_index|slope_ohe    |numeric_features            |scaled_numeric_features                                                                             |features                                                                                                                                    |target|\n",
      "+---+--------+-------------+----+----------+-------------+-----+-----------+-------------+----------------------------+----------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------+------+\n",
      "|1.0|3.0     |(5,[3],[1.0])|6.0 |2.0       |(4,[2],[1.0])|3.0  |2.0        |(4,[2],[1.0])|[63.0,145.0,233.0,150.0,2.3]|[0.9346032586984205,0.7491157101964772,-0.2759776060833338,0.017464957251029093,1.067164164714716]  |(18,[3,7,11,13,14,15,16,17],[1.0,1.0,1.0,0.9346032586984205,0.7491157101964772,-0.2759776060833338,0.017464957251029093,1.067164164714716]) |0     |\n",
      "|4.0|0.0     |(5,[0],[1.0])|3.0 |0.0       |(4,[0],[1.0])|2.0  |1.0        |(4,[1],[1.0])|[67.0,160.0,286.0,108.0,1.5]|[1.3766051183057972,1.593576866301058,0.7433005490921232,-1.8132735028269908,0.38113005882668416]   |(18,[0,5,10,13,14,15,16,17],[1.0,1.0,1.0,1.3766051183057972,1.593576866301058,0.7433005490921232,-1.8132735028269908,0.38113005882668416])  |2     |\n",
      "|4.0|0.0     |(5,[0],[1.0])|7.0 |1.0       |(4,[1],[1.0])|2.0  |1.0        |(4,[1],[1.0])|[67.0,120.0,229.0,129.0,2.6]|[1.3766051183057972,-0.6583195499778243,-0.352904259304123,-0.8979042727879808,1.3244269544227285]  |(18,[0,6,10,13,14,15,16,17],[1.0,1.0,1.0,1.3766051183057972,-0.6583195499778243,-0.352904259304123,-0.8979042727879808,1.3244269544227285]) |1     |\n",
      "|3.0|1.0     |(5,[1],[1.0])|3.0 |0.0       |(4,[0],[1.0])|3.0  |2.0        |(4,[2],[1.0])|[37.0,130.0,250.0,187.0,3.5]|[-1.938408828749528,-0.0953454459081037,0.050960670105020305,1.6302583625578562,2.0962153235467644] |(18,[1,5,11,13,14,15,16,17],[1.0,1.0,1.0,-1.938408828749528,-0.0953454459081037,0.050960670105020305,1.6302583625578562,2.0962153235467644])|0     |\n",
      "|2.0|2.0     |(5,[2],[1.0])|3.0 |0.0       |(4,[0],[1.0])|1.0  |0.0        |(4,[0],[1.0])|[41.0,130.0,204.0,172.0,1.4]|[-1.4964069691421513,-0.0953454459081037,-0.8336958419340555,0.9764231982442776,0.29537579559068006]|(18,[2,5,9,13,14,15,16,17],[1.0,1.0,1.0,-1.4964069691421513,-0.0953454459081037,-0.8336958419340555,0.9764231982442776,0.29537579559068006])|0     |\n",
      "+---+--------+-------------+----+----------+-------------+-----+-----------+-------------+----------------------------+----------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show important intermediate and final columns for verification\n",
    "processed_df.select(\n",
    "    \"cp\", \"cp_index\", \"cp_ohe\",\n",
    "    \"thal\", \"thal_index\", \"thal_ohe\",\n",
    "    \"slope\", \"slope_index\", \"slope_ohe\",\n",
    "    \"numeric_features\", \"scaled_numeric_features\",\n",
    "    \"features\", \"target\"\n",
    ").show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c09227d1-bdfe-4b27-978a-feadd9289bf3",
   "metadata": {},
   "source": [
    "# convert a few \"features\" vectors to lists for easy reading (Pandas)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "38ce9621-46a3-4ccc-8f94-da90f0306e28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                            features_list  target\n",
      "  [0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.9346032586984205, 0.7491157101964772, -0.2759776060833338, 0.017464957251029093, 1.067164164714716]       0\n",
      "   [1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.3766051183057972, 1.593576866301058, 0.7433005490921232, -1.8132735028269908, 0.38113005882668416]       2\n",
      "  [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.3766051183057972, -0.6583195499778243, -0.352904259304123, -0.8979042727879808, 1.3244269544227285]       1\n",
      " [0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, -1.938408828749528, -0.0953454459081037, 0.050960670105020305, 1.6302583625578562, 2.0962153235467644]       0\n",
      "[0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, -1.4964069691421513, -0.0953454459081037, -0.8336958419340555, 0.9764231982442776, 0.29537579559068006]       0\n"
     ]
    }
   ],
   "source": [
    "# convert a few \"features\" vectors to lists for easy reading (Pandas)\n",
    "sample_pdf = processed_df.select(\"features\", \"target\").limit(5).toPandas()\n",
    "sample_pdf[\"features_list\"] = sample_pdf[\"features\"].apply(lambda v: list(v))\n",
    "\n",
    "print(sample_pdf[[\"features_list\", \"target\"]].to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dfc3b06-7d84-46fc-a799-f46163829f1a",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c030d4b1-9da2-4ae8-92d0-e798fe25e3b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------------------------------------------------------------------------------------------------+------+-----+\n",
      "|features                                                                                                                                   |target|label|\n",
      "+-------------------------------------------------------------------------------------------------------------------------------------------+------+-----+\n",
      "|(18,[3,7,11,13,14,15,16,17],[1.0,1.0,1.0,0.9346032586984205,0.7491157101964772,-0.2759776060833338,0.017464957251029093,1.067164164714716])|0     |0.0  |\n",
      "|(18,[0,5,10,13,14,15,16,17],[1.0,1.0,1.0,1.3766051183057972,1.593576866301058,0.7433005490921232,-1.8132735028269908,0.38113005882668416]) |2     |2.0  |\n",
      "|(18,[0,6,10,13,14,15,16,17],[1.0,1.0,1.0,1.3766051183057972,-0.6583195499778243,-0.352904259304123,-0.8979042727879808,1.3244269544227285])|1     |1.0  |\n",
      "+-------------------------------------------------------------------------------------------------------------------------------------------+------+-----+\n",
      "only showing top 3 rows\n",
      "\n",
      "Train rows: 251, Test rows: 46\n"
     ]
    }
   ],
   "source": [
    "# 0) Imports & setup\n",
    "from pyspark.ml.classification import LogisticRegression, DecisionTreeClassifier, RandomForestClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.ml import PipelineModel\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "# Ensure the label column exists and is double\n",
    "# If you already have a 'label' column from StringIndexer, skip creating it.\n",
    "if \"label\" not in processed_df.columns:\n",
    "    processed_df = processed_df.withColumn(\"label\", col(\"target\").cast(\"double\"))\n",
    "\n",
    "# Quick check\n",
    "processed_df.select(\"features\", \"target\", \"label\").show(3, truncate=False)\n",
    "\n",
    "# Train/test split\n",
    "train_df, test_df = processed_df.randomSplit([0.8, 0.2], seed=42)\n",
    "print(f\"Train rows: {train_df.count()}, Test rows: {test_df.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aea1d8f2-eb22-4a6a-9a9f-bfbb9e435f04",
   "metadata": {},
   "source": [
    "# Define evaluators\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "282ad327-65b7-43dd-9911-ff5c41e6f78c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Evaluators (multiclass)\n",
    "evaluator_acc = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "evaluator_f1  = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"f1\")\n",
    "evaluator_precision = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"weightedPrecision\")\n",
    "evaluator_recall = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"weightedRecall\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "294c4264-c252-46a3-9bde-9905d3c40dca",
   "metadata": {},
   "source": [
    "# Logistic Regression (multinomial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7e1e0d37-4a2c-492e-b084-d7e5548722cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Logistic Regression (multinomial) ===\n",
      "Accuracy: 0.6739  F1: 0.6575  Precision: 0.7853  Recall: 0.6739\n"
     ]
    }
   ],
   "source": [
    "# 2) Logistic Regression (multinomial) - baseline\n",
    "lr = LogisticRegression(featuresCol=\"features\", labelCol=\"label\", maxIter=100, family=\"multinomial\")\n",
    "\n",
    "lr_model = lr.fit(train_df)\n",
    "pred_lr = lr_model.transform(test_df)\n",
    "\n",
    "acc_lr = evaluator_acc.evaluate(pred_lr)\n",
    "f1_lr  = evaluator_f1.evaluate(pred_lr)\n",
    "prec_lr = evaluator_precision.evaluate(pred_lr)\n",
    "recall_lr = evaluator_recall.evaluate(pred_lr)\n",
    "\n",
    "print(\"=== Logistic Regression (multinomial) ===\")\n",
    "print(f\"Accuracy: {acc_lr:.4f}  F1: {f1_lr:.4f}  Precision: {prec_lr:.4f}  Recall: {recall_lr:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00022c3c-ef36-41d4-912d-346125520d08",
   "metadata": {},
   "source": [
    "### Logistic Regression performed the best among all models.\n",
    "### With an accuracy of 67.39% and precision of 78.53%, \n",
    "### it predicts heart disease cases quite accurately and minimizes false positives.\n",
    "### Recall of 67.39% means it correctly identified around two-thirds of actual patients.\n",
    "### Overall, it’s a reliable baseline model with balanced performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6751bcf8-fc14-4be6-8b3e-cb6b21c844be",
   "metadata": {},
   "source": [
    "# Decision Tree — interpretable tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f8bdcb9b-8132-4feb-904c-54bc5cabcf1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Decision Tree ===\n",
      "Accuracy: 0.5217  F1: 0.4977  Precision: 0.4789  Recall: 0.5217\n"
     ]
    }
   ],
   "source": [
    "# 3) Decision Tree\n",
    "dt = DecisionTreeClassifier(featuresCol=\"features\", labelCol=\"label\", maxDepth=6)  # tune maxDepth as needed\n",
    "dt_model = dt.fit(train_df)\n",
    "pred_dt = dt_model.transform(test_df)\n",
    "\n",
    "acc_dt = evaluator_acc.evaluate(pred_dt)\n",
    "f1_dt  = evaluator_f1.evaluate(pred_dt)\n",
    "prec_dt = evaluator_precision.evaluate(pred_dt)\n",
    "recall_dt = evaluator_recall.evaluate(pred_dt)\n",
    "\n",
    "print(\"=== Decision Tree ===\")\n",
    "print(f\"Accuracy: {acc_dt:.4f}  F1: {f1_dt:.4f}  Precision: {prec_dt:.4f}  Recall: {recall_dt:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b0b76c4-f6d0-4a04-b0ca-42e9e65f0dee",
   "metadata": {},
   "source": [
    "### Decision Tree achieved 52.17% accuracy and relatively lower precision (47.89%).\n",
    "### This indicates the model made more incorrect positive predictions.\n",
    "### Recall of 52.17% shows it detected only about half of the real patients.\n",
    "### It may be overfitting or not generalizing well to unseen data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72fc06dc-2423-4ff2-9a56-df6285779367",
   "metadata": {},
   "source": [
    "# Random Forest — stronger general-purpose model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bc55e8bd-09ae-4519-bf27-7ccea06ef9e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Random Forest ===\n",
      "Accuracy: 0.5652  F1: 0.5120  Precision: 0.4763  Recall: 0.5652\n"
     ]
    }
   ],
   "source": [
    "# 4) Random Forest\n",
    "rf = RandomForestClassifier(featuresCol=\"features\", labelCol=\"label\", numTrees=100, maxDepth=8, seed=42)\n",
    "rf_model = rf.fit(train_df)\n",
    "pred_rf = rf_model.transform(test_df)\n",
    "\n",
    "acc_rf = evaluator_acc.evaluate(pred_rf)\n",
    "f1_rf  = evaluator_f1.evaluate(pred_rf)\n",
    "prec_rf = evaluator_precision.evaluate(pred_rf)\n",
    "recall_rf = evaluator_recall.evaluate(pred_rf)\n",
    "\n",
    "print(\"=== Random Forest ===\")\n",
    "print(f\"Accuracy: {acc_rf:.4f}  F1: {f1_rf:.4f}  Precision: {prec_rf:.4f}  Recall: {recall_rf:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f371c389-7a61-4fe8-8b88-e09cb025723c",
   "metadata": {},
   "source": [
    "### Random Forest performed slightly better than Decision Tree with 56.52% accuracy.\n",
    "### Its precision (47.63%) and recall (56.52%) are moderate,\n",
    "### meaning it balanced false positives and false negatives but wasn’t very strong in either.\n",
    "### It provided more stable predictions than a single tree, but still below Logistic Regression.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e4e1fd-a717-4bef-9530-86ea2ec9313a",
   "metadata": {},
   "source": [
    "# Compare all models side-by-side\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a89936a9-6d76-4e2c-9f53-411ccca126e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model\t\tAccuracy\tF1\tPrecision\tRecall\n",
      "LogisticRegression\t0.6739\t0.6575\t0.7853\t0.6739\n",
      "DecisionTree     \t0.5217\t0.4977\t0.4789\t0.5217\n",
      "RandomForest     \t0.5652\t0.5120\t0.4763\t0.5652\n"
     ]
    }
   ],
   "source": [
    "# 5) Summary table\n",
    "results = [\n",
    "    (\"LogisticRegression\", acc_lr, f1_lr, prec_lr, recall_lr),\n",
    "    (\"DecisionTree\", acc_dt, f1_dt, prec_dt, recall_dt),\n",
    "    (\"RandomForest\", acc_rf, f1_rf, prec_rf, recall_rf)\n",
    "]\n",
    "\n",
    "print(\"\\nModel\\t\\tAccuracy\\tF1\\tPrecision\\tRecall\")\n",
    "for name, acc, f1, pr, rc in results:\n",
    "    print(f\"{name:17s}\\t{acc:.4f}\\t{f1:.4f}\\t{pr:.4f}\\t{rc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6e71fe79-4a8d-41de-88c8-407a678865a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest feature importances (index:value):\n",
      "Index 0: 0.061656\n",
      "Index 1: 0.019207\n",
      "Index 2: 0.012085\n",
      "Index 3: 0.016177\n",
      "Index 5: 0.087374\n",
      "Index 6: 0.065758\n",
      "Index 7: 0.014821\n",
      "Index 9: 0.030555\n",
      "Index 10: 0.031643\n",
      "Index 11: 0.013251\n",
      "Index 13: 0.122554\n",
      "Index 14: 0.113605\n",
      "Index 15: 0.117953\n",
      "Index 16: 0.155046\n",
      "Index 17: 0.138314\n"
     ]
    }
   ],
   "source": [
    "# 7) Feature importances (vector indices -> importance)\n",
    "importances = rf_model.featureImportances  # this is a SparseVector-like structure\n",
    "print(\"Random Forest feature importances (index:value):\")\n",
    "for idx, imp in enumerate(importances):\n",
    "    if imp > 0:\n",
    "        print(f\"Index {idx}: {imp:.6f}\")\n",
    "\n",
    "# NOTE: to interpret index -> original variable, use the mapping we built earlier\n",
    "# (i.e., lengths of cp_ohe, thal_ohe, slope_ohe and numeric_cols order)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a064fa8c-cad1-4f39-b6e7-130de52fae07",
   "metadata": {},
   "source": [
    "The Random Forest model shows that features like oldpeak, thalach, and cholesterol have the highest importance scores, meaning they strongly influence the model’s prediction of heart disease.\n",
    "Categorical features such as chest pain type (cp) and thal also contribute but to a lesser extent.\n",
    "Overall, the model relies more on numeric medical indicators than on categorical ones to identify heart disease."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4f3d8599-b4e6-47ff-ba54-810c75c31d60",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\anaconda3\\S.K\\envs\\py310env\\lib\\site-packages\\pyspark\\sql\\context.py:158: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Confusion Matrix for Logistic Regression ===\n",
      "[[25.  0.  1.  0.  0.]\n",
      " [ 4.  3.  2.  0.  0.]\n",
      " [ 2.  0.  1.  0.  0.]\n",
      " [ 1.  0.  3.  2.  1.]\n",
      " [ 1.  0.  0.  0.  0.]]\n",
      "\n",
      "=== Confusion Matrix for Decision Tree ===\n",
      "[[20.  5.  1.  0.  0.]\n",
      " [ 2.  4.  1.  1.  1.]\n",
      " [ 2.  1.  0.  0.  0.]\n",
      " [ 2.  3.  2.  0.  0.]\n",
      " [ 1.  0.  0.  0.  0.]]\n",
      "\n",
      "=== Confusion Matrix for Random Forest ===\n",
      "[[24.  2.  0.  0.  0.]\n",
      " [ 4.  2.  3.  0.  0.]\n",
      " [ 2.  1.  0.  0.  0.]\n",
      " [ 2.  1.  4.  0.  0.]\n",
      " [ 1.  0.  0.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "\n",
    "def confusion_matrix(model, test_df, model_name):\n",
    "    # Make predictions on the test data\n",
    "    predictions = model.transform(test_df)\n",
    "    \n",
    "    # Convert to (prediction, label) RDD\n",
    "    pred_and_labels = predictions.select(\"prediction\", \"label\").rdd.map(lambda x: (float(x[0]), float(x[1])))\n",
    "    \n",
    "    # Create metrics object\n",
    "    metrics = MulticlassMetrics(pred_and_labels)\n",
    "    \n",
    "    # Confusion matrix\n",
    "    cm = metrics.confusionMatrix().toArray()\n",
    "    print(f\"\\n=== Confusion Matrix for {model_name} ===\")\n",
    "    print(cm)\n",
    "    return predictions\n",
    "\n",
    "# Generate confusion matrices for all three trained models\n",
    "lr_predictions = confusion_matrix(lr_model, test_df, \"Logistic Regression\")\n",
    "dt_predictions = confusion_matrix(dt_model, test_df, \"Decision Tree\")\n",
    "rf_predictions = confusion_matrix(rf_model, test_df, \"Random Forest\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de065ca6-37ac-48ef-91a8-ba18e6334fa7",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "\n",
    "Most predictions are correct for class 0 (normal) → 25 correct.\n",
    "Some confusion between classes 1–3, but overall performs better than others.\n",
    "Interpretation: Logistic Regression predicts “no disease” and mild disease well — best overall among three models.\n",
    "\n",
    "## Decision Tree\n",
    "\n",
    "Many wrong predictions — noticeable confusion between classes (off-diagonal values).\n",
    "Only a few classes correctly identified.\n",
    "Interpretation: Decision Tree is overfitting or not generalizing well — performance weaker than Logistic Regression.\n",
    "\n",
    "## Random Forest\n",
    "\n",
    "Slightly better than Decision Tree — more correct predictions for class 0.\n",
    "Still confused between neighboring disease levels.\n",
    "Interpretation: Random Forest improves stability a bit, but still less accurate than Logistic Regression for this dataset.\n",
    "\n",
    "## Summary:\n",
    "Logistic Regression → best balanced model.\n",
    "Random Forest → moderate, some confusion.\n",
    "Decision Tree → weakest, confused across classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2149e4dc-764a-4275-94ae-2d1983ebfdbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+---+--------+-----+---+-------+-------+-----+-------+-----+---+----+------+\n",
      "| age|sex| cp|trestbps| chol|fbs|restecg|thalach|exang|oldpeak|slope| ca|thal|target|\n",
      "+----+---+---+--------+-----+---+-------+-------+-----+-------+-----+---+----+------+\n",
      "|63.0|1.0|1.0|   145.0|233.0|1.0|    2.0|  150.0|  0.0|    2.3|  3.0|0.0| 6.0|     0|\n",
      "|67.0|1.0|4.0|   160.0|286.0|0.0|    2.0|  108.0|  1.0|    1.5|  2.0|3.0| 3.0|     2|\n",
      "|67.0|1.0|4.0|   120.0|229.0|0.0|    2.0|  129.0|  1.0|    2.6|  2.0|2.0| 7.0|     1|\n",
      "|37.0|1.0|3.0|   130.0|250.0|0.0|    0.0|  187.0|  0.0|    3.5|  3.0|0.0| 3.0|     0|\n",
      "|41.0|0.0|2.0|   130.0|204.0|0.0|    2.0|  172.0|  0.0|    1.4|  1.0|0.0| 3.0|     0|\n",
      "+----+---+---+--------+-----+---+-------+-------+-----+-------+-----+---+----+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "processed_df =df_dropna.select(\"*\")\n",
    "processed_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f0b3fc87-ad01-4efa-9a42-76820f593423",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+\n",
      "|target|label|\n",
      "+------+-----+\n",
      "|     0|  0.0|\n",
      "|     2|  2.0|\n",
      "|     1|  1.0|\n",
      "+------+-----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# 0) Ensure label exists\n",
    "if \"label\" not in processed_df.columns:\n",
    "    processed_df = processed_df.withColumn(\"label\", col(\"target\").cast(\"double\"))\n",
    "processed_df.select(\"target\", \"label\").show(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "83e77f6a-0847-4df9-84bf-7b9f915c947d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "# 7) Model (use best model; logistic regression here)\n",
    "lr = LogisticRegression(labelCol=\"label\", featuresCol=\"features\", maxIter=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "879f620d-ccc5-4a6d-8978-dc824c8aacb4",
   "metadata": {},
   "source": [
    "Defines a Logistic Regression Classifier, an ensemble of decision trees that vote on the final prediction. It’s robust, handles both numeric and categorical data, and reduces overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4c3a9615-94ca-4485-9866-257b18be0374",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# 6) Build Pipeline, fit & transform, inspect results\n",
    "stages = indexers + encoders + [numeric_assembler, scaler, final_assembler,lr]\n",
    "pipeline = Pipeline(stages=stages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dda9b090-55d7-4768-83b8-7475df2f7693",
   "metadata": {},
   "source": [
    "Creates a sequence of operations:\n",
    "\n",
    "Assemble features\n",
    "\n",
    "Scale them\n",
    "\n",
    "Train model\n",
    "\n",
    "pipeline.fit() will execute these steps in order\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96ea928a-f340-446b-8148-c5ba5575b3b8",
   "metadata": {},
   "source": [
    "# Train/test split (deterministic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "25d571aa-b20b-4026-a500-2a952369dc65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train rows: 251 Test rows: 46\n"
     ]
    }
   ],
   "source": [
    "# 9) Train/test split (deterministic)\n",
    "train_df, test_df = processed_df.randomSplit([0.8, 0.2], seed=42)\n",
    "print(\"Train rows:\", train_df.count(), \"Test rows:\", test_df.count())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d485f150-6553-4e69-a993-e06a73a82c82",
   "metadata": {},
   "source": [
    "Splits your dataset into training (80%) and testing (20%) sets for fair evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a9aed39c-fef3-44d0-be61-36b479401eed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Pipeline model saved successfully at: D:\\Py_Spark\\Model_2\\heart_pipeline_v1\n"
     ]
    }
   ],
   "source": [
    "# 10) Fit pipeline (this learns index mappings and scaler params)\n",
    "pipeline_model = pipeline.fit(train_df)\n",
    "\n",
    "save_path = r\"D:\\Py_Spark\\Model_2\\heart_pipeline_v1\"  # you can pick another folder if you prefer\n",
    "pipeline_model.write().overwrite().save(save_path)\n",
    "print(\"✅ Pipeline model saved successfully at:\", save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ed603d-8258-41ae-9f4f-a0028f8840a6",
   "metadata": {},
   "source": [
    "Learns scaling parameters (mean, std)\n",
    "\n",
    "Trains the Logistic Regression model using training data\n",
    "\n",
    "Returns a PipelineModel that contains all fitted components."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3da26a4d-c075-4bea-a255-f246e97e423f",
   "metadata": {},
   "source": [
    "#  Loding the ML Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "76f00277-32fe-4912-b066-3ab4dfa96b7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Pipeline loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import PipelineModel\n",
    "pm = PipelineModel.load(r\"D:\\Py_Spark\\Model_2\\heart_pipeline_v1\")\n",
    "print(\"✅ Pipeline loaded successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7249f764-19fb-46da-b2aa-353e0abb4406",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+\n",
      "|label|prediction|\n",
      "+-----+----------+\n",
      "|0.0  |0.0       |\n",
      "|1.0  |1.0       |\n",
      "|0.0  |0.0       |\n",
      "|3.0  |3.0       |\n",
      "|0.0  |0.0       |\n",
      "+-----+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 11) Predict & evaluate on test set\n",
    "predictions = pipeline_model.transform(test_df)\n",
    "predictions.select(\"label\", \"prediction\").show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f0d7e8f-c8f2-488c-b42a-0feeb2d196b8",
   "metadata": {},
   "source": [
    "Applies all preprocessing + model automatically to the test data, producing:\n",
    "\n",
    "label: actual value\n",
    "\n",
    "prediction: predicted class\n",
    "\n",
    "probability: model confidence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f7df575-0713-4a3b-b752-b3328405ce50",
   "metadata": {},
   "source": [
    "# Model Evealuvation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1b43ff64-3c6a-40ac-bd7a-739a568b4cf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.6739130434782609\n",
      "F1       : 0.65750020469991\n",
      "Precision: 0.7853378505552419\n",
      "Recall   : 0.6739130434782608\n"
     ]
    }
   ],
   "source": [
    "eval_acc = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "eval_f1  = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"f1\")\n",
    "eval_prec= MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"weightedPrecision\")\n",
    "eval_rec = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"weightedRecall\")\n",
    "\n",
    "print(\"Accuracy :\", eval_acc.evaluate(predictions))\n",
    "print(\"F1       :\", eval_f1.evaluate(predictions))\n",
    "print(\"Precision:\", eval_prec.evaluate(predictions))\n",
    "print(\"Recall   :\", eval_rec.evaluate(predictions))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "508ae833-c984-4dd8-89b8-2f7605b18984",
   "metadata": {},
   "source": [
    "Calculates and prints the performance metrics — accuracy, precision, recall, and F1 score — to evaluate the model.\n",
    "\n",
    "Saves the entire trained pipeline (assembler, scaler, model) to disk for later use — no retraining needed.\n",
    "\n",
    "Reloads your trained pipeline. We can use it directly on new datasets that have the same columns — this ensures consistent preprocessing and predictions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73087caf-6716-44f8-8dea-929879047d03",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 (py310env)",
   "language": "python",
   "name": "py310env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
